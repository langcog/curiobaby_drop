---
#title: "Modeling children's intrinsically-motivated curiosity about object interactions"
#title: "Drop it like it's hot: Modeling children's curiosity about physical interactions"
#title: "A drop in the bucket: Progress towards modeling children's curiosity about physical interactions"
title: "What if you drop that? Modeling children's curiosity about physical interactions"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
     Curiosity is a fundamental driver of human behavior, and yet because of its open-ended nature 
     and the wide variety of behaviors it inspires in different contexts, it is remarkably difficult 
     to study in a laboratory context. A promising approach to developing and testing theories of 
     curiosity is to instantiate them in artificial agents that are able to act and explore in a 
     simulated environment, and then compare the behavior of these agents to humans exploring the 
     same stimuli. Here we propose a new experimental paradigm for examining children's--and curious
     AI agents'--curiosity about objects' physical interactions, using a task that is both open-ended
     enough to allow room for curiosity, but also constrained enough to make detailed behavioral 
     comparisons. We compare the choices of 51 children (2-6 years of age) to those of agents implementing 
     different types of curiosity, and find broad exploration, but also systematic preferences ...
    
keywords: >
    curiosity; novel objects; object interactions; intuitive physics
    
output: cogsci2016::cogsci_paper
#    includes:
#      in_header: preamble.tex
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
require(here)
require(tidyverse)
require(tidyboot)
library(apa) # for generating APA-style text to report statistical tests
library(ggpubr)
```

```{r, load-data}
tower = read.csv2("../analysis/human-data/curiobaby_drop-data - cool tower.csv", sep=',', header=T, stringsAsFactors=F)
raw = read.csv2("../analysis/human-data/curiobaby_drop-data - drop exp.csv", sep=',', header=T, stringsAsFactors=F)
hum = subset(raw, Exclude!="Y")

hum = hum[-which(hum$StimSet=="bc" & hum$DropChoice=="bowl and dumbbell"),] # chose two - exclude

s_tr <- hum %>% group_by(SID, Age_Group) %>% summarise(max=max(Trial)) %>% arrange(max)
age = table(s_tr$Age_Group)
```

# Introduction 

<!-- Curiosity is important... end with core question about what gives rise to curiosity. -->
Curiosity is a key driver of everyday behaviors, from children's exploratory play with a new toy to adults browsing the internet, we spend a surprising amount of time actively pursuing information about our world. 
Curiosity may even serve as the impetus for all learning: William James (1899) considered it to be "the impulse towards better cognition", a belief that is echoed in Piaget's [-@Piaget1952] theory of development, which holds that children actively construct their knowledge through physical exploration of the world.
From this view, children and even infants [@Gopnik2009] can be seen as "little scientists" who use curiosity to select causal interventions that help them learn about the physical world.
However, a major outstanding difficulty in curiosity research: there is no integrative, testable theory of the mechanisms and basis for curiosity [@Kidd2015].
More concretely, what relationship between stimuli and a learner's knowledge states engender curiosity, and how does this motivate a learner's choice of actions?

<!-- desiderata for advancing theories of curiosity -->

A fully-specified, testable theory of curiosity would minimally require three components.
First, it is necessary to have a precise mechanism specifying how stimuli inspire curiosity. 
Although some early verbal theories of curiosity acknowledge that novel stimuli demand our (and other animals') attention [e.g., @Berlyne1954], borne out in empirical data [e.g., @Fantz1964], but modern theories hold that learners may prefer to attend to stimuli that are "moderately discrepant" in comparison to our mental representations [@Kinney1976].
These theories hold that curiosity is inspired by awareness of a knowledge gap [@Loewenstein1994], and that as we gain experience with classes of stimuli our preferences will tend toward more complex stimuli as our mental representations mature [@Dember1957].
Empirical work has verified that infants are sensitive to the level of complexity of stimuli and prefer to attend to stimuli of intermediate complexity [rather than simple or overly-complex; @Kidd2012; @Kidd2014].
Thus, similar to other drives (e.g., hunger), curiosity is sated as relevant information is gained and the knowledge gap is closed, but defining the perceptual and knowledge-based representations and how they are compared is an open question.

Second, a testable theory of curiosity requires a detailed measure of curious behavior that records which interventions learners select in order to gain information about the properties of objects and laws that govern the physical world. 
People's curiosity-driven behavior comes in such a variety of contexts and utilizing such a wide repertoire of actions that it is quite difficult to choose an appropriate paradigm, particularly because the places where curiosity is most manifest tend to be those open-ended environments that are hardest to model -- an inviting playground, a surprisingly complex device, or a novel research question. 
Past studies of infant's physical reasoning ability have used looking time to measure their knowledge about physical events, given that infant's attend longer to surprising events [e.g., objects appearing to pass through a wall; @Stahl2015].
Behavioral investigations of older children's exploratory play have also used simple attention duration measures to measure curiosity, for example offering children a single complex toy with a number of functions that can be discovered [e.g., @Cook2011; @Bonawitz2012; @Gweon2014], and additionally tally the number of discovered functions.
However, these attention and discovery measures do not quantify the overt actions that children choose, and typically do not offer more than one or two target objects for exploration.

A third desiderata for a testable theory of curiosity is a linking function for evaluating the correspondence between the predictions of our computational theory and the observed patterns of action selection that people show in the same physical task. 
Although behavioral studies of young children have found that they preferentially play with toys in order to learn the causal structure [@Schulz2007; @Gweon2014], there has been no detailed formal account of what their play actions are, or how they are selected in a given situation.
Infant studies of surprisal to various physical events involving occlusion, containment, and covering of objects has revealed that infants are unsurprised by many physical interactions [@Baillargeon2007], but do not specify or predict what physical interventions infants may make in order to learn more about the properties of objects and the laws of physics.
Recent computational models have proposed linking functions relating.. in simulation [@Schmidhuber2010; @Oudeyer2013], but the predictions of these models have not yet been directly compared to human behavior in the same task to reveal how curiosity drives people's choice of causal interventions. 

<!-- High-level statement of our approach -->
In this paper we propose a new paradigm that allows us to directly compare children's curiosity about physical interactions with that of an aritifical agent implementing different functional forms of curiosity.
Our approach is to coordinate detailed measurement of stimulus and action selection with the construction of task-performing computational models evaluated on the same task and using the same metric, with the goal of developing a more robust, unified, and testable theory of curiosity's role in cognitive development.

<!-- What we've learned from prior theoretical work, and how it falls short -->

<!-- A limitation of past theories of curiosity is that they are large part verbal descriptions, without formal, testable, mechanisms. -->
<!-- An exception to this: @Schmidhuber2010 and @Oudeyer2013 --similar but different, and critically not evaluated against same children's behavior in the same situations -->

<!-- What we've learned from prior behavioral work, and how our approach is different -->

<!-- Looking time people: @Baillargeon2007 and other studies of infants' surprisal at physical events (occlusion, containment,  -->
<!-- Ullman -- not about curiosity, but about intuitive physics learning from observation in simplified 2D domains (e.g., colliding hockey pucks) --  -->


<!-- Highlight main contributions of the paper -->


<!-- One rich domain that young children are in the midst of learning about is the properties and affordances of physical objects.  -->
<!-- A study of 9- to 16-month-olds found that infants' violation of expectations about a novel toy's hidden properties (e.g., making a sound) drives them to explore similar toys for longer [@Baldwin1993]. -->

## Computational Models of Curiosity

Building on prior work that showed deep neural networks are capable of learning forward and inverse physical dynamics (i.e., "intuitive physics") from images when given the ability to "poke" the objects in the scene [@Agrawal2016], we use and extend a framework for constructing intrinsically-motivated artificial agents introduced in @Haber2018learning, described in the following. 

(Model description or just teaser here?)

(maybe make the below a joint "Paradigm" section, then human behavior, model behavior and comparison)

# Experiment

## Method

### Participants
Participants were `r length(unique(raw$SID))` children recruited from the Children's Discovery Museum of San Jose and Bing Nursery School.
Participant exclusions were made based on cases where i) the participant did not complete more than half of the study play session or ii) the parent did not consent for video recording of study. 
After exclusions, results from `r length(unique(hum$SID))` were analyzed, including `r age[1]` 2-year-olds, `r age[2]` 3-year-olds, `r age[3]` 4-year-olds, `r age[4]` 5-year-olds, and `r age[5]` 6-year-olds.\footnote{The collected sample differed from our planned sample size of 16 each of 3-, 4-, and 5-year-olds due to availability of participants (see preregistration: https://osf.io/37qvb/).}

### Materials

```{r object-sets}
shapes = c("bowl","cone","dumbbell","octahedron","pentagon","pipe","pyramid","torus","trig prism")

a = c("pyramid", "torus", "trig prism")
b = c("cone", "octahedron", "pipe")
c = c("bowl", "dumbbell", "pentagon")

hum <- hum %>% 
  mutate(DropSet=ifelse(StimSet %in% c("ba","ca"), "A", ifelse(StimSet %in% c("ab","cb"), "B", "C")),
         TargetSet=ifelse(StimSet %in% c("ab","ac"), "A", ifelse(StimSet %in% c("ba","bc"), "B", "C")))
```

Stimuli were 3D-printed plastic objects produced using Blender 3D-modeling software. 
The nine objects, depicted in Figure 1, were bowl, cone, dumbbell, octahedron, pentagon (pentagonal prism), pipe, pyramid, torus, and triangular prism.
The printed objects were all yellow, rigid plastic material and designed to fit comfortably in a child's hand (dimension range: 3.8-10.1 cm). 

The set of 9 objects were divided into 3 subsets (A = {pyramid, torus, triangular prism}, B = {cone, octahedron, pipe}, and C = {bowl, dumbbell, pentagon}). 
These subsets served as sets (A, B, C) of target or drop objects in six successive blocks of 2 trials, making a total of 12 trials per participant.
For example, in Order 1, in the first block of 2 trials, set A served as the target objects from which the child chose to drop on the target objects (set B).
The target:drop block sequence for Order 1 was A:B, B:C, C:A, A:C, B:A, C:B. The sequence for Order 2 was the reverse of Order 1: C:B, B:A, A:C, C:A, B:C, A:B. Participants were assigned to condition in counterbalanced order.

Target objects were placed in a circular bin (25 in diameter x 10 in height). 
The bin was divided with tape into sections of equal area, and one target object from the appropriate set was placed in the center of each third. 
Drop objects were presented to participants on a table at approximately eye level.

```{r fig1-sets, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3.25, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Sets of 3D objects used for dropping and as targets."}
img <- png::readPNG("figs/drop_sets.png")
grid::grid.raster(img)
```

### Procedure

After the parent provided informed consent, children were assigned to Order 1 or Order 2. Participants were assigned to condition in counterbalanced order.
We introduced children to a set of 3D-printed toy objects ("blocks"). 
The child played a game where they could pick which of three blocks to drop in a bin containing three other blocks, to see what happens. 
An example trial is illustrated in Figure 2.
We then swapped target/drop blocks, based on assigned order sequence, and asked the child to do the same selection and dropping a dozen times. 
Finally, we asked the child to build a "cool" tower with any of the toy blocks for about one minute.

Based on piloting, we estimated the activity would would only require five minutes to complete. 
In both conditions, a video camera was used to record the play session from an angle above the bin, to show child's block selection and drop location as well as child's completed tower. 
After child notified the researcher that they were finished building their tower, the session was completed and camera was turned off. 

```{r fig2-task, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Example trial in which the participant chose to drop the pyramid from set A on the pentagonal prism in the target set (B)."}
img <- png::readPNG("figs/DropTaskFigure.png")
grid::grid.raster(img)
```

### Drop Coding Procedure

Each session's video was manually coded for drop object choice and target location per trial.
Drop choice was defined as the participant's selection of one of the three available drop set objects, to be dropped into the bin containing the set of the three potential target objects. 
The bin was divided into three wedge-shaped sections of equal area, demarcated with a thin black line.
One of the target set's objects was placed in the middle of each section of the bin. 
Target location was defined as either i) the object the dropped object collided with, ii) a collision with empty space inside the bin, or iii) falling outside the bin.
If the dropped object collided with empty space, we also recorded the target object it landed closest to. 
We also recorded events where the dropped object bounced from empty space or a target object then collided with other target object(s), including the number of collisions and identity of the target(s) hit.

Trials were excluded if the child i) dropped an object outside the bin, ii) touched target object(s) prior to a drop, iii) dropped an object during set change, iv) selected more than one object to drop, or iv) dropped (or threw) object from too high (>1.5 m). 
Trials were also excluded if the experimenter made a mistake (e.g., used the wrong stimulus set or repeated a trial more than twice).


## Results

```{r chance-comparison}
hum$TargetSpace = ifelse(hum$DropLocation=="space", hum$WhichSpace, hum$DropLocation)

report_chisq <- function(X) {
  # e.g. X^2 (2, N = 88) = 2.1, p = .35
  pval = round(X$p.value, 3)
  if(pval==0) { 
    pval = ", p<.001" 
  } else {
    pval = paste0(", p=",pval)
  }
  return(paste0("($X^2$(", X$parameter, ", N=", sum(X$expected), ") = ", round(X$statistic,2), pval, ")"))
}

# see if children's choices (per subset) are random
drop_ch = table(hum$StimSet, hum$DropChoice)
# chisq.test(rbind(colSums(drop_ch[,a]), rep(sum(drop_ch[,a]/3), 3)) )

Xa_drop = chisq.test( colSums(drop_ch[,a]) ) # p<.001
Xb_drop = chisq.test( colSums(drop_ch[,b]) ) # p=.01
Xc_drop = chisq.test( colSums(drop_ch[,c]) ) # p=.056

target_ch = table(hum$StimSet, hum$DropLocation)

Xa_targ = chisq.test( colSums(target_ch[,a]) ) # n.s.
Xb_targ = chisq.test( colSums(target_ch[,b]) ) # n.s.
Xc_targ = chisq.test( colSums(target_ch[,c]) ) # n.s.

targetloc_ch = table(hum$StimSet, hum$TargetSpace)
Xa_tloc = chisq.test( colSums(targetloc_ch[,a]) ) # n.s.
Xb_tloc = chisq.test( colSums(targetloc_ch[,b]) ) # n.s.
Xc_tloc = chisq.test( colSums(targetloc_ch[,c]) ) # n.s.

# 'misses' by age
space_age = table(subset(hum, DropLocation=="space")$Age_Group)
all_age = table(hum$Age_Group)
#Xmiss_age = chisq.test(space_age, p=all_age / sum(all_age)) # n.s.
prop_space = sum(space_age) / sum(all_age)

age_s <- hum %>% 
  group_by(SID, Age_Exact) %>%
  summarise(miss=sum(DropLocation=="space"), N=n()) %>%
  mutate(prop_miss = miss / N)
age_miss = cor.test(age_s$prop_miss, as.numeric(age_s$Age_Exact))

# now check DropChoice x TargetLocation interactions
# maybe we do chisq.test on particular combinations? e.g., for DropSet A and TargetSet B?
dA = subset(hum, DropSet=="A" & TargetSet=="B")
dto = table(dA$DropChoice, dA$DropLocation)[,1:3]
#dto = table(hum$DropChoice, hum$DropLocation)[,c(1:4,6:8,10:11)] # drop 'outside' and 'space'
Xdto = chisq.test(dto)

```

We first examine children's choice of drop objects and target objects to determine if children were choosing randomly, or had consistent preferences for some objects. 
We used chi-square test of independence on participants' drop choices from each set of objects (A, B, C).
Participants' drop choices from set A significantly differed from chance `r report_chisq(Xa_drop)`, as did drop choices from set B `r report_chisq(Xb_drop)`.
Participants' drop choices from set C did not significantly differ from chance `r report_chisq(Xc_drop)`.
Table 1 shows a summary of participants' choice of drop objects per set.
It is apparent that from set A, participants preferentially chose to drop the torus rather than the triangular prism (trig prism) or pyramid. 
For set B, children more often avoided the octahedron, and instead chose the pipe or cone.
Set C showed more equal rates of drop choice, but the dumbbell was somewhat more popular than the pentagonal prism (pentagon).

What objects did children target with their drops?
First, we noted that children very often did not hit any target object: on `r round(prop_space, 2)*100`% of trials, participants' dropped over empty space.
It is not possible to determine whether children intentionally missed the available target objects, or whether their misses were errors due to noise in fine motor control.
If the misses were just due to poor motor control, one might expect the number of misses by age to decrease, as older children should have better motor control.
However, there was no significant correlation between children's individual proportion of "missed" targets and their age (`r cor_apa(age_miss, format="rmarkdown", print=F)`), which may suggest that dropping objects on empty space may be of interest to children of all ages, and not solely determined by a lack of fine motor control.
Thus, we first analyze drops where participants' dropped objects directly hit the target objects.
Chi-square tests of children's target collisions revealed that they did not significantly differ from chance for set A `r report_chisq(Xa_targ)`, B `r report_chisq(Xb_targ)`, or C `r report_chisq(Xc_targ)`.
Table 2 shows a summary of participants' choice of target objects per set.

Finally, we examine a more lenient analysis children's choice of target location (i.e., the identity of the closest target), regardless of whether there was a collision of drop and target objects.
Chi-square tests of children's target collisions once more found that they did not significantly differ from chance for set A `r report_chisq(Xa_tloc)`, B `r report_chisq(Xb_tloc)`, or C `r report_chisq(Xc_tloc)`.
Table 3 shows a summary of participants' choice of target locations per set.

In summary, children showed some consistent preferences in their choice of objects to drop from each set, but no systematic preference for targeting particular objects.
We now investigate whether there were interactions of drop objects and targets that children found particularly appealing. [ToDo: big chisq test?]

Finally, we analyze how exploratory children were in their responses: across the 12 trials, what proportion of the 9 available objects did each child utilize as drop objects? What proportion of the objects were targeted?

```{r table1-drop, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
get_table_string <- function(tableX, set) {
  tt = sort(colSums(tableX[,set]))
  tstr = paste(paste0(names(tt), " (", tt, "),"), collapse=' ')
  tstr = gsub('.{1}$', '', tstr)
  return(tstr)
}
dA = get_table_string(drop_ch, a)
dB = get_table_string(drop_ch, b)
dC = get_table_string(drop_ch, c)

tabl <- tribble(~Set, ~`Drop Object (N)`, 
                "A", dA,
                "B", dB,
                "C", dC)
t1 <- xtable::xtable(tabl, caption="Children's drop object choices by set.")
#tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), 
#                       caption = "This table prints across one column.")
print(t1, type="latex", comment = F, table.placement = "H")
```

```{r table2-target, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tA = get_table_string(target_ch, a)
tB = get_table_string(target_ch, b)
tC = get_table_string(target_ch, c)

tabl2 <- tribble(~Set, ~`Target Object (N)`, 
                "A", tA,
                "B", tB,
                "C", tC)
t2 <- xtable::xtable(tabl2, caption="Children's target object choices by set.")
print(t2, type="latex", comment = F, table.placement = "H")
```

```{r table3-targetloc, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tA = get_table_string(targetloc_ch, a)
tB = get_table_string(targetloc_ch, b)
tC = get_table_string(targetloc_ch, c)

tabl3 <- tribble(~Set, ~`Target Location (N)`, 
                "A", tA,
                "B", tB,
                "C", tC)
t3 <- xtable::xtable(tabl3, caption="Children's target location choices by set.")
print(t3, type="latex", comment = F, table.placement = "H")
```

```{r drop-choice, include=F, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.4, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Relative frequency of drop object choice as a function of stimulus set."}
ch_cond <- hum %>% 
  group_by(StimSet, DropChoice) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

ch_cond$StimSet <- factor(ch_cond$StimSet, levels = c("ba","ca","ab","cb","ac","bc"))
fig_drop <- ggplot(ch_cond, aes(x=StimSet, y=freq, shape=DropChoice, color=DropChoice)) + # , size=n
  geom_point(alpha=.8) +
  scale_shape_manual(values=10:19) +
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Stimulus Set") + ylab("Rate of Drop Object Choice") + ylim(0,.66) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  geom_hline(yintercept=.33, lty='dashed') + labs(shape="Object", color="Object")
#print(fig_drop)
```

```{r target-hit, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.4, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Relative frequency of targeted object as a function of stimulus set."}
hum$Target = hum$DropLocation
ch_cond <- hum %>% 
  filter(Target!="outside") %>%
  group_by(StimSet, Target) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

ch_cond$Target <- factor(ch_cond$Target, levels = c(shapes, "space"))
fig_targ_hit <- ggplot(ch_cond, aes(x=StimSet, y=freq, shape=Target, color=Target)) + # , size=n
  geom_point(alpha=.8) +
  scale_shape_manual(values=10:20) +
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Stimulus Set") + ylab("Rate of Target Object Hit") + ylim(0,.66) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  geom_hline(yintercept=.33, lty='dashed')
#print(fig_targ_hit)
```

```{r target-space, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.4, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Relative frequency of targeted object as a function of stimulus set."}

ch_cond <- hum %>% 
  filter(is.element(TargetSpace, shapes)) %>%
  group_by(StimSet, TargetSpace) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

ch_cond$TargetSpace <- factor(ch_cond$TargetSpace, levels = shapes)
fig_targ_space <- ggplot(ch_cond, aes(x=StimSet, y=freq, shape=TargetSpace, color=TargetSpace)) + # , size=n
  geom_point(alpha=.8) +
  scale_shape_manual(values=10:19) +
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Stimulus Set") + ylab("Rate of Targeted Location") + ylim(0,.66) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  geom_hline(yintercept=.33, lty='dashed')
#print(fig_targ_space)
```



```{r combined-fig, fig.env = "figure*", fig.pos = "h", fig.align="center", fig.width=7, fig.height=3.2, set.cap.width=T, num.cols.cap=2, fig.cap = "Relative frequency of objects 1) chosen as objects for dropping, 2) hit as targets (pink dots represent hitting empty space), and 3) whose drop location was closest to a target object, as a function of target/drop set pairing."}

ggarrange(fig_drop, fig_targ_hit, fig_targ_space, nrow=1, 
          #labels = c("Drop Choice", "Targeted Object Hit", "Targeted Location"),
          common.legend=T, legend="bottom")
```


Figure 4 shows participants' mean proportion of unique objects dropped as a function of age.
Children of all ages sampled approximately equal proportions of the objects for dropping--roughly 70%, which is close to the 75% that would be expected if they were selected by chance (9 unique object occurring across 12 trials).

```{r unique-drop-objects, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Proportion of unique objects selected by participants. Error bars show boostrapped 95\\% confidence intervals."}
# 12 trials, 9 objects; random selection would be 9/12 = 75%...
uniq_s <- hum %>% group_by(SID, Age_Group) %>%
  summarise(DropObjs = n_distinct(DropChoice), N=n(), prop=DropObjs/N) 

uniq <- uniq_s %>% group_by(Age_Group) %>%
  tidyboot_mean(prop) 
  
ggplot(uniq, aes(x=Age_Group, y=mean)) + geom_point() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Age (years)") + ylab("Unique Objects Chosen") + ylim(0,1) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  geom_hline(yintercept=.75, lty='dashed')
```

```{r drop-by-target, include=F, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Heatmap of participants' drop object choices by target location."}
ch_loc <- hum %>% filter(DropLocation!="outside" & DropLocation!="space") %>%
  group_by(DropChoice, DropLocation) %>% 
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))

ggplot(ch_loc, aes(x=DropLocation, y=DropChoice)) + geom_tile(aes(fill=freq)) + 
  scale_fill_gradient(low = "white", high = "steelblue") +
  xlab("Target Location") + ylab("Drop Object") +
  ggthemes::theme_few()
```

# Model

## Baseline Curiosity Policy

## Antagonistic Curiosity Policy

## Results


## Comparison with Children



# Discussion 



# Acknowledgements

This work was funded by HAI seed grant #. 
We thank X and Y for helpful comments.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
