---
#title: "Modeling children's intrinsically-motivated curiosity about object interactions"
#title: "Drop it like it's hot: Modeling children's curiosity about physical interactions"
#title: "A drop in the bucket: Progress towards modeling children's curiosity about physical interactions"
title: "What if you drop that? Modeling children's curiosity about physical interactions"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
     Curiosity is a fundamental driver of human behavior, and yet because of its open-ended nature 
     and the wide variety of behaviors it inspires in different contexts, it is remarkably difficult 
     to study in a laboratory context. A promising approach to developing and testing theories of 
     curiosity is to instantiate them in artificial agents that are able to act and explore in a 
     simulated environment, and then compare the behavior of these agents to humans exploring the 
     same stimuli. Here we propose a new experimental paradigm for examining children's--and curious
     AI agents'--curiosity about objects' physical interactions, using a task that is both open-ended
     enough to allow room for curiosity, but also constrained enough to make detailed behavioral 
     comparisons. We compare the choices of 51 children (2-6 years of age) to those of agents implementing 
     different types of curiosity, and find broad exploration, but also systematic preferences ...
    
keywords: >
    curiosity; novel objects; object interactions; intuitive physics
    
output: cogsci2016::cogsci_paper
#    includes:
#      in_header: preamble.tex
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
require(here)
require(tidyverse)
require(tidyboot)
library(apa) # for generating APA-style text to report statistical tests
library(ggpubr)
```

```{r, load-data}
raw = read.csv2("../analysis/human-data/curiobaby_drop-data - drop exp.csv", sep=',', header=T, stringsAsFactors=F)
hum = subset(raw, Exclude!="Y")

hum = hum[-which(hum$StimSet=="bc" & hum$DropChoice=="bowl and dumbbell"),] # chose two - exclude

s_tr <- hum %>% group_by(SID, Age_Group) %>% summarise(max=max(Trial)) %>% arrange(max)
age = table(s_tr$Age_Group)
```

# Introduction 

Curiosity is a fundamental part of our motivation, driving everyday behaviors that we spend a surprising amount of time pursuing, from browsing the internet to reading novels, watching the news, and many other forms of information seeking.
Curiosity is so strongly linked to learning that William James (1899) considered it to be "the impulse towards better cognition", a description that is echoed in Piaget's [-@Piaget1952] theory of development, which holds that children actively construct their knowledge through physical exploration of the world.
Taking this view of children [even infants: @Gopnik2009] as "little scientists", researchers have credited curiosity for inspiring behaviors ranging from novelty-seeking to exploration and play, and yet there is a major outstanding difficulty in curiosity research: there is no integrative theory of the mechanisms and basis for curiosity [@Kidd2015].
What combinations of stimuli and knowledge states inspire curiosity? 
By what mechanism does curiosity motivate and drive action selection, and influence subsequent learning and behavior?

Although several theories of curiosity (surveyed briefly below) partially address these issues, a gold standard for testing these theories would require a fully-instantiated theory of curiosity in an artificial agent whose behavior can be directly compared to that of humans.
However, a further difficulty of this approach is selecting a task that adequately captures the general nature of curiosity: curiosity-driven human behavior is so wide-ranging that it is difficult to systematically compare to presently limited artificial agents.
Behavioral investigations of children's exploratory play have often offered a single complex toy with a number of functions that can be discovered [e.g., @Cook2011; @Bonawitz2012; @Gweon2014], but the real world offers many more degrees of freedom.
The places where curiosity is most manifest tend to be those open-ended environments that are hardest to model -- an inviting playground, a surprisingly complex device, or a novel research question. 
In this paper we propose a new paradigm that allows us to directly compare children's curiosity about physical interactions with that of an aritifical agent implementing different functional forms of curiosity.
Below, we give a brief overview of theories of curiosity, relevant developmental empirical findings, and computational models instantiating curiosity.

## Theories of Curiosity

@Berlyne1954's influential theory of curiosity distinguished between perceptual curiosity, a desire to seek out novel stimuli (that decreases with familiarity), and epistemic curiosity, a desire to acquire knowledge. 
Berlyne additionally distinguished specific versus diversive curiosity, where the former is a drive for a particular piece of information, and the latter is a general motivation for stimulation.
More recent views of curiosity further limit the definition to information-seeking that is intrinsically-motivated, not linked to any obvious external reward [@Loewenstein1994; @Oudeyer2007].
However, it is often difficult for observers to determine whether any given decision is intrinsically- or extrinsically-motivated.
Modern theories also hold that the function of curiosity is inspired by awareness of a knowledge gap, and functions to drive behaviors that will result in learning [@Loewenstein1994].
Similar to other drives (e.g., hunger), curiosity is sated as relevant information is gained and the knowledge gap is closed.
Although these theories give us some sense of which situations and knowledge states should engender curiosity, both the mechanism and how actions are selected 

## Related Behavioral Work

Although the question of what inspires a child's curiosity is difficult to determine, it's generally agreed that curiosity indicates awareness of a knowledge gap that the learner is interested in filling [@Loewenstein1994].
Even young infants exhibit a preference to attend to novel stimuli over more familiar stimuli [e.g., @Fantz1964], and when surprising events do occur it has been shown to stimulate infants' exploration and subsequent learning [@Stahl2015].
Moreover, infants are sensitive to the level of complexity of stimuli, preferring to attend to those of intermediate complexity (rather than simple or overly-complex)--a bias which may help them optimize their information gain [@Kidd2012; @Kidd2014].
As theorized [@Piaget1952], exploratory play has been shown to be an opportunity for young children to learn [e.g., generalization: @Sim2017, causal structure: @Schulz2007; @Gweon2014].

One rich domain that young children are in the midst of learning about is the properties and affordances of physical objects.


## Computational Models of Curiosity

[Sentence or two on other approaches, e.g. @Oudeyer2013; @Schmidhuber2010]

Building on prior work that showed deep neural networks are capable of learning forward and inverse physical dynamics (i.e., "intuitive physics") from images when given the ability to "poke" the objects in the scene [@Agrawal2016], we use and extend a framework for constructing intrinsically-motivated artificial agents introduced in @Haber2018learning, described in the following. 

(Model description or just teaser here?)

(maybe make the below a joint "Paradigm" section, then human behavior, model behavior and comparison)

# Experiment

## Method

### Participants
Participants were `r length(unique(raw$SID))` children recruited from the Children's Discovery Museum of San Jose and Bing Nursery School.
Participant exclusions were made based on cases where i) the participant did not complete more than half of the study play session or ii) the parent did not consent for video recording of study. 
After exclusions, results from `r length(unique(hum$SID))` were analyzed, including `r age[1]` 2-year-olds, `r age[2]` 3-year-olds, `r age[3]` 4-year-olds, `r age[4]` 5-year-olds, and `r age[5]` 6-year-olds.^[1]

[1]: The collected sample differed from our planned sample size of 16 each of 3-, 4-, and 5-year-olds due to availability of participants (see preregistration: https://osf.io/37qvb/).

### Materials

```{r object-sets}
shapes = c("bowl","cone","dumbbell","octahedron","pentagon","pipe","pyramid","torus","trig prism")

a = c("pyramid", "torus", "trig prism")
b = c("cone", "octahedron", "pipe")
c = c("bowl", "dumbbell", "pentagon")
```

Stimuli were 3D-printed plastic objects produced using Blender 3D-modeling software. 
The nine objects, depicted in Figure 1, were bowl, cone, dumbbell, octahedron, pentagon (pentagonal prism), pipe, pyramid, torus, and triangular prism.
The printed objects were all yellow, rigid plastic material and designed to fit comfortably in the childâ€™s hand (dimension range: 3.8-10.1 cm). 

The set of 9 objects were divided into 3 subsets (A = {pyramid, torus, triangular prism}, B = {cone, octahedron, pipe}, and C = {bowl, dumbbell, pentagon}). 
These subsets served as sets (A, B, C) of target or drop objects in six successive blocks of 2 trials, making a total of 12 trials per participant.
For example, in Order 1, in the first block of 2 trials, set A served as the target objects from which the child chose to drop on the target objects (set B).
The target:drop block sequence for Order 1 was A:B, B:C, C:A, A:C, B:A, C:B. The sequence for Order 2 was the reverse of Order 1: C:B, B:A, A:C, C:A, B:C, A:B. Participants were assigned to condition in counterbalanced order.

Target objects were placed in a circular bin (25 in diameter x 10 in height). 
The bin was divided with tape into sections of equal area, and one target object from the appropriate set was placed in the center of each third. 
Drop objects were presented to participants on a table at approximately eye level.

```{r fig1-sets, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3.25, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Sets of 3D objects used for dropping and as targets."}
img <- png::readPNG("figs/drop_sets.png")
grid::grid.raster(img)
```

### Procedure

After the parent provided informed consent, children were assigned to Order 1 or Order 2. Participants were assigned to condition in counterbalanced order.
We introduced children to a set of 3D-printed toy objects ("blocks"). 
The child played a game where they could pick which of three blocks to drop in a bin containing three other blocks, to see what happens. 
An example trial is illustrated in Figure 2.
We then swapped target/drop blocks, based on assigned order sequence, and asked the child to do the same selection and dropping a dozen times. 
Finally, we asked the child to build a "cool" tower with any of the toy blocks for about one minute.

Based on piloting, we estimated the activity would would only require five minutes to complete. 
In both conditions, a video camera was used to record the play session from an angle above the bin, to show child's block selection and drop location as well as child's completed tower. 
After child notified the researcher that they were finished building their tower, the session was completed and camera was turned off. 

```{r fig2-task, fig.env = "figure", fig.pos = "h", fig.align='center', fig.width=3, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Example trial in which the participant chose to drop the pyramid from set A on the pentagonal prism in the target set (B)."}
img <- png::readPNG("figs/DropTaskFigure.png")
grid::grid.raster(img)
```

### Drop Coding Procedure

Each session's video was manually coded for object drop choice and drop location. Drop choice is defined as the child's single selection of three drop objects to drop into the bin containing three different objects. 
The bin is divided into three sections of equal area. 
Each target object resides in one of these three areas. 
Drop location was defined as the site of collision of drop object at the base of the bin. 
Drop locations include either i) direct collision with one of three target objects, ii) collision in an empty space surrounding select target object, or iii) outside the bin. 

If the dropped object collided with empty space, we recorded the target object it landed closest to. 
We also recorded events where the dropped object bounced from empty space or a target object then collided with other target object(s), including the number of collisions and identity of the target(s) hit.

Trials were excluded if the child i) dropped an object outside the bin, ii) touched target object(s) prior to a drop, iii) dropped an object during set change, iv) selected more than one object to drop, or iv) dropped (or threw) object from too high (>1.5 m). 
Trials were also excluded if the experimenter made a mistake (e.g., used the wrong stimulus set or repeated a trial more than twice).


## Results

```{r chance-comparison}
hum$TargetSpace = ifelse(hum$DropLocation=="space", hum$WhichSpace, hum$DropLocation)

report_chisq <- function(X) {
  # e.g. X^2 (2, N = 88) = 2.1, p = .35
  pval = round(X$p.value, 3)
  if(pval==0) { 
    pval = ", p<.001" 
  } else {
    pval = paste0(", p=",pval)
  }
  return(paste0("($X^2$(", X$parameter, ", N=", sum(X$expected), ") = ", round(X$statistic,2), pval, ")"))
}

# see if children's choices (per subset) are random
drop_ch = table(hum$StimSet, hum$DropChoice)
# chisq.test(rbind(colSums(drop_ch[,a]), rep(sum(drop_ch[,a]/3), 3)) )

Xa_drop = chisq.test( colSums(drop_ch[,a]) ) # p<.001
Xb_drop = chisq.test( colSums(drop_ch[,b]) ) # p=.01
Xc_drop = chisq.test( colSums(drop_ch[,c]) ) # p=.056

target_ch = table(hum$StimSet, hum$DropLocation)

Xa_targ = chisq.test( colSums(target_ch[,a]) ) # n.s.
Xb_targ = chisq.test( colSums(target_ch[,b]) ) # n.s.
Xc_targ = chisq.test( colSums(target_ch[,c]) ) # n.s.

targetloc_ch = table(hum$StimSet, hum$TargetSpace)
Xa_tloc = chisq.test( colSums(targetloc_ch[,a]) ) # n.s.
Xb_tloc = chisq.test( colSums(targetloc_ch[,b]) ) # n.s.
Xc_tloc = chisq.test( colSums(targetloc_ch[,c]) ) # n.s.

# 'misses' by age
space_age = table(subset(hum, DropLocation=="space")$Age_Group)
all_age = table(hum$Age_Group)
#Xmiss_age = chisq.test(space_age, p=all_age / sum(all_age)) # n.s.
prop_space = sum(space_age) / sum(all_age)

age_s <- hum %>% 
  group_by(SID, Age_Exact) %>%
  summarise(miss=sum(DropLocation=="space"), N=n()) %>%
  mutate(prop_miss = miss / N)
age_miss = cor.test(age_s$prop_miss, as.numeric(age_s$Age_Exact))
```

We first examine children's choice of drop objects and target objects to determine if children were choosing randomly, or had consistent preferences for some objects. 
We used chi-square test of independence on participants' drop choices from each set of objects (A, B, C).
Participants' drop choices from set A significantly differed from chance `r report_chisq(Xa_drop)`, as did drop choices from set B `r report_chisq(Xb_drop)`.
Participants' drop choices from set C did not significantly differ from chance `r report_chisq(Xc_drop)`.
Table 1 shows a summary of participants' choice of drop objects per set.
It is apparent that from set A, participants preferentially chose to drop the torus rather than the triangular prism (trig prism) or pyramid. 
For set B, children more often avoided the octahedron, and instead chose the pipe or cone.
Set C showed more equal rates of drop choice, but the dumbbell was somewhat more popular than the pentagonal prism (pentagon).

What objects did children target with their drops?
First, we noted that children very often did not hit any target object: on `r round(prop_space, 2)*100`% of trials, participants' dropped over empty space.
It is not possible to determine whether children intentionally missed the available target objects, or whether their misses were errors due to noise in fine motor control.
If the misses were just due to poor motor control, one might expect the number of misses by age to decrease, as older children should have better motor control.
However, there was no significant correlation between children's individual proportion of "missed" targets and their age (`r cor_apa(age_miss, format="rmarkdown", print=F)`), which may suggest that dropping objects on empty space may be of interest to children of all ages, and not solely determined by a lack of fine motor control.
Thus, we first analyze drops where participants' dropped objects directly hit the target objects.
Chi-square tests of children's target collisions revealed that they did not significantly differ from chance for set A `r report_chisq(Xa_targ)`, B `r report_chisq(Xb_targ)`, or C `r report_chisq(Xc_targ)`.
Table 2 shows a summary of participants' choice of target objects per set.

Finally, we examine a more lenient analysis children's choice of target location (i.e., the identity of the closest target), regardless of whether there was a collision of drop and target objects.
Chi-square tests of children's target collisions once more found that they did not significantly differ from chance for set A `r report_chisq(Xa_tloc)`, B `r report_chisq(Xb_tloc)`, or C `r report_chisq(Xc_tloc)`.
Table 3 shows a summary of participants' choice of target locations per set.

In summary, children showed some consistent preferences in their choice of objects to drop from each set, but no systematic preference for targeting particular objects.
We now investigate whether there were interactions of drop objects and targets that children found particularly appealing. [ToDo: big chisq test?]

Finally, we analyze how exploratory children were in their responses: across the 12 trials, what proportion of the 9 available objects did each child utilize as drop objects? What proportion of the objects were targeted?

```{r table1-drop, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
get_table_string <- function(tableX, set) {
  tt = sort(colSums(tableX[,set]))
  tstr = paste(paste0(names(tt), " (", tt, "),"), collapse=' ')
  tstr = gsub('.{1}$', '', tstr)
  return(tstr)
}
dA = get_table_string(drop_ch, a)
dB = get_table_string(drop_ch, b)
dC = get_table_string(drop_ch, c)

tabl <- tribble(~Set, ~`Drop Object (N)`, 
                "A", dA,
                "B", dB,
                "C", dC)
t1 <- xtable::xtable(tabl, caption="Children's drop object choices by set.")
#tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), 
#                       caption = "This table prints across one column.")
print(t1, type="latex", comment = F, table.placement = "H")
```

```{r table2-target, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tA = get_table_string(target_ch, a)
tB = get_table_string(target_ch, b)
tC = get_table_string(target_ch, c)

tabl2 <- tribble(~Set, ~`Target Object (N)`, 
                "A", tA,
                "B", tB,
                "C", tC)
t2 <- xtable::xtable(tabl2, caption="Children's target object choices by set.")
print(t2, type="latex", comment = F, table.placement = "H")
```

```{r table3-targetloc, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tA = get_table_string(targetloc_ch, a)
tB = get_table_string(targetloc_ch, b)
tC = get_table_string(targetloc_ch, c)

tabl3 <- tribble(~Set, ~`Target Location (N)`, 
                "A", tA,
                "B", tB,
                "C", tC)
t3 <- xtable::xtable(tabl3, caption="Children's target location choices by set.")
print(t3, type="latex", comment = F, table.placement = "H")
```

```{r drop-choice, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.4, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Relative frequency of drop object choice as a function of stimulus set."}
ch_cond <- hum %>% 
  group_by(StimSet, DropChoice) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

ch_cond$StimSet <- factor(ch_cond$StimSet, levels = c("ba","ca","ab","cb","ac","bc"))
fig_drop <- ggplot(ch_cond, aes(x=StimSet, y=freq, shape=DropChoice, color=DropChoice)) + # , size=n
  geom_point(alpha=.8) +
  scale_shape_manual(values=10:19) +
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Stimulus Set") + ylab("Rate of Drop Object Choice") + ylim(0,.66) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  geom_hline(yintercept=.33, lty='dashed') + labs(shape="Object", color="Object")
#print(fig_drop)
```

```{r target-hit, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.4, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Relative frequency of targeted object as a function of stimulus set."}
hum$Target = hum$DropLocation
ch_cond <- hum %>% 
  filter(Target!="outside") %>%
  group_by(StimSet, Target) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

ch_cond$Target <- factor(ch_cond$Target, levels = c(shapes, "space"))
fig_targ_hit <- ggplot(ch_cond, aes(x=StimSet, y=freq, shape=Target, color=Target)) + # , size=n
  geom_point(alpha=.8) +
  scale_shape_manual(values=10:20) +
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Stimulus Set") + ylab("Rate of Target Object Hit") + ylim(0,.66) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  geom_hline(yintercept=.33, lty='dashed')
#print(fig_targ_hit)
```

```{r target-space, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3.4, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Relative frequency of targeted object as a function of stimulus set."}

ch_cond <- hum %>% 
  filter(is.element(TargetSpace, shapes)) %>%
  group_by(StimSet, TargetSpace) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) 

ch_cond$TargetSpace <- factor(ch_cond$TargetSpace, levels = shapes)
fig_targ_space <- ggplot(ch_cond, aes(x=StimSet, y=freq, shape=TargetSpace, color=TargetSpace)) + # , size=n
  geom_point(alpha=.8) +
  scale_shape_manual(values=10:19) +
  #geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Stimulus Set") + ylab("Rate of Targeted Location") + ylim(0,.66) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() + 
  geom_hline(yintercept=.33, lty='dashed')
#print(fig_targ_space)
```



```{r combined-fig, fig.env = "figure*", fig.pos = "h", fig.align="center", fig.width=7, fig.height=3.2, set.cap.width=T, num.cols.cap=2, fig.cap = "Relative frequency of objects 1) chosen as objects for dropping, 2) hit as targets, and 3) whose drop location was closest to a target object, as a function of target/drop set pairing."}

ggarrange(fig_drop, fig_targ_hit, fig_targ_space, nrow=1, 
          #labels = c("Drop Choice", "Targeted Object Hit", "Targeted Location"),
          common.legend=T, legend="bottom")
```


Figure 4 shows participants' mean proportion of unique objects dropped as a function of age.
Children of all ages sampled approximately equal proportions of the objects for dropping--roughly 70%, which is close to the 75% that would be expected if they were selected by chance (9 unique object occurring across 12 trials).

```{r unique-drop-objects, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=2.5, set.cap.width=T, num.cols.cap=1, fig.cap = "Proportion of unique objects selected by participants. Error bars show boostrapped 95\\% confidence intervals."}
# 12 trials, 9 objects; random selection would be 9/12 = 75%...
uniq_s <- hum %>% group_by(SID, Age_Group) %>%
  summarise(DropObjs = n_distinct(DropChoice), N=n(), prop=DropObjs/N) 

uniq <- uniq_s %>% group_by(Age_Group) %>%
  tidyboot_mean(prop) 
  
ggplot(uniq, aes(x=Age_Group, y=mean)) + geom_point() +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Age (years)") + ylab("Unique Objects Chosen") + ylim(0,1) + 
  langcog::scale_fill_solarized() + ggthemes::theme_few() +
  geom_hline(yintercept=.75, lty='dashed')
```

# Model

## Baseline Curiosity Policy

## Antagonistic Curiosity Policy

## Results


## Comparison with Children



# Discussion 



# Acknowledgements

This work was funded by HAI seed grant #. 
We thank X and Y for helpful comments.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
