---
title: "curiobaby_drop model analysis"
author: "George Kachergis"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
require(tidyverse)
require(here)
```

## Load data

```{r, load-data}


colMeans(mdat[,1:22])
# obj_final_position_invstd_objects=drop = -0.2980893  should that be negative?

load_data <- function(fname) {
  raw <- read.table(here(paste0("models/",fname,".csv")), sep=',')
  cnames <- raw[1,]
  mdat <- raw[2:nrow(raw),]
  names(mdat) = cnames
  for(c in 1:(ncol(mdat)-3)) {
    mdat[,c] = as.numeric(mdat[,c])
    if(min(mdat[,c]) < 0) mdat[,c] = mdat[,c] + min(mdat[,c])
    print(paste("shifting",c,"so minimum is 0"))
  }
  mdat$trial = rep(1:20, each=2)
  # right now, each trial is 2 rows, 
  # uniquely identified with drop_object x condition
  #mdat %>% distinct(drop_object, condition)

  mdat <- mdat %>% rename(drop = drop_object,
                          target = target_object,
                          relation = condition) %>%
    mutate(drop = replace(drop, drop=="triangular_prism", "trig prism"),
         target = replace(target, target=="triangular_prism", "trig prism"))
  return(mdat)
}

# just 100 drops per trial
mdat100 <- load_data("model_stats")

mdat <- load_data("model_stats250")

sort(colMeans(mdat[,1:43]))
# mean of obj_final_position_invstd_objects=drop is negative..
# mean of avg_len is 235, and len_std is 45 (much larger than typical feature mean on order of .1 - 1.0)
# scale these cols? (only matters for their combination)
mdat$avg_len = mdat$avg_len / max(mdat$avg_len)
mdat$len_std = mdat$len_std / max(mdat$len_std)

# engineer some features: 
# combine target + drop target features
mdat_eng <- mdat %>%
  rowwise() %>%
  mutate(obj_final_position_std = mean(`obj_final_position_std_objects=drop`,
                                       `obj_final_position_std_objects=target`),
         normed_velocity_std_after_first_collision_objects = 
           mean(`normed_velocity_std_after_first_collision_objects=drop`,
                `normed_velocity_std_after_first_collision_objects=target`),
         avg_final_radius_objects = mean(`avg_final_radius_objects=drop`, 
                                         `avg_final_radius_objects=target`),
         avg_max_radius_objects = mean(`avg_max_radius_objects=drop`,
                                       `avg_max_radius_objects=target`),
         max_radius_std_objects = mean(`max_radius_std_objects=drop`,
                                       `max_radius_std_objects=target`),
         support_combo = mean(support_probability, support_std, 
                              `support_response_sharpness_C=1`, # add support_response_linearity_r or _pv ?
                              `support_response_sharpness_accuracy_C=1`),
         support_sharpness_mean = mean(`support_response_sharpness_C=1e-05`,
                                       `support_response_sharpness_C=0.0001`,
                                       `support_response_sharpness_C=0.001`,
                                       `support_response_sharpness_C=0.01`,
                                       `support_response_sharpness_C=0.1`,
                                       `support_response_sharpness_C=1`,
                                       `support_response_sharpness_C=10.0`,
                                       `support_response_sharpness_C=100.0`,
                                       `support_response_sharpness_C=1000.0`,
                                       `support_response_sharpness_C=10000.0`,
                                       `support_response_sharpness_C=100000.0`),
         support_sharpness_accuracy_mean = mean(
            `support_response_sharpness_accuracy_C=1e-05`,
             `support_response_sharpness_accuracy_C=0.0001`,
             `support_response_sharpness_accuracy_C=0.001`,
             `support_response_sharpness_accuracy_C=0.01`,
             `support_response_sharpness_accuracy_C=0.1`,
             `support_response_sharpness_accuracy_C=1`,
             `support_response_sharpness_accuracy_C=10.0`,
             `support_response_sharpness_accuracy_C=100.0`,
             `support_response_sharpness_accuracy_C=1000.0`,
             `support_response_sharpness_accuracy_C=10000.0`,
             `support_response_sharpness_accuracy_C=100000.0`),
         support_response_linearity_mean = mean(support_response_linearity_pv, 
                                                support_response_linearity_r),
         len_combo = mean(avg_len, len_std, len_inverse_sharpe_ratio)
         ) %>%
  select(obj_final_position_std, 
         normed_velocity_std_after_first_collision_objects, 
         avg_final_radius_objects,
         avg_max_radius_objects,
         max_radius_std_objects,
         support_combo, 
         support_sharpness_mean,
         support_sharpness_accuracy_mean,
         len_combo,
         drop, target, relation)



# selects given feature and returns wide df that can be compared with behavioral data
get_feature_df <- function(mdat, fname) {
  fdf <- mdat[,c(fname,"drop","target","relation")] %>%
    pivot_wider(id_cols = c("drop","relation"), names_from = "target", values_from = fname) %>%
    arrange(relation,drop)
}

```

# Fit to adult choice proportions

```{r}
load(file=here("paper/processed_data.RData")) 

# ToDo: hold out half the subjects and CV on other half
adult_trial_agg <- adult_long %>% group_by(relation, drop, target) %>% # alt, 
  summarize(chose_target=mean(chose_target, na.rm=T)) %>% # , n=n()
  arrange(relation, desc(chose_target))

#hum_ad <- adult_trial_agg %>%
#  pivot_wider(id_cols = c("drop","relation"), names_from = "target", values_from = chose_target) %>%
#  arrange(relation,drop)

softmax <- function(weights, beta=1, smooth=1e-7) {
  weights = weights + smooth
  probs = (weights / sum(weights)) # normalize first
  num <- exp(beta * probs)
  return(num / sum(num))
}
# softmax(c(.7,.3), beta=2)

get_model_target_choice_props <- function(feat_df, hum_df, beta) {
  hum_df <- hum_df %>% arrange(relation, drop) # same order as model df
  hum_df$Model_targ_prop = 0
  for(i in 1:nrow(hum_df)) {
    mod_cols = which(!is.na(feat_df[i,1:ncol(feat_df)]))[3:4]
    mod_choice = softmax(feat_df[i,mod_cols], beta=beta)
    hum_df[i,]$Model_targ_prop = as.numeric(mod_choice[hum_df[i,]$target] / sum(mod_choice))
  }
  return(hum_df)
}

evalFit <- function(feat_df, hum_df, beta) {
  hum_df <- get_model_target_choice_props(feat_df, hum_df, beta)
  #r = with(hum_df, cor(chose_target, Model_targ_prop))
  mse = with(hum_df, sum((chose_target - Model_targ_prop)^2)) / nrow(hum_df)
  return(mse) # return negative cor since DEoptim minimizes
}

require(DEoptim)

feat_names = names(mdat)[1:(ncol(mdat)-4)]
# for each feature, want to fit beta to maximize correlation between human and model choices
#feat_df <- get_feature_df(mdat, feat_names[1])

child_trial_agg <- child_long %>% group_by(relation, drop, target) %>% # alt, 
  summarize(chose_target=mean(chose_target, na.rm=T), n=n())

adult_fits = list()
child_fits = list()
for(feat in feat_names) {
  feat_df <- get_feature_df(mdat, feat)
  #paste(feat, evalFit(feat_df, adult_trial_agg, 1))
  adult_fits[[feat]] = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=adult_trial_agg)
  child_fits[[feat]] = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=child_trial_agg)
}
```

## Now try engineered features

```{r adult-eng-feats}
feat_names2 = names(mdat_eng)[1:(ncol(mdat_eng)-3)]

for(feat in feat_names2) {
  feat_df <- get_feature_df(mdat_eng, feat)
  #paste(feat, evalFit(feat_df, adult_trial_agg, 1))
  adult_fits[[feat]] = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=adult_trial_agg)
  child_fits[[feat]] = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=child_trial_agg)
}

```



```{r}
adult_mfit <- tibble()
child_mfit <- tibble()

for(f in names(adult_fits)) {
  adult_mfit = bind_rows(adult_mfit, c(feature=f, 
                                       beta=as.numeric(unlist(adult_fits[[f]]$optim$bestmem)), 
                                       mse=adult_fits[[f]]$optim$bestval))
  child_mfit = bind_rows(child_mfit, c(feature=f, 
                                       beta=as.numeric(unlist(child_fits[[f]]$optim$bestmem)), 
                                       mse=child_fits[[f]]$optim$bestval))
}

adult_mfit <- adult_mfit %>% 
  mutate(beta = as.numeric(beta),
         mse = as.numeric(mse)) %>%
  arrange(mse)

```

```{r children}
child_mfit <- child_mfit %>% 
  mutate(beta = as.numeric(beta),
         mse = as.numeric(mse)) %>%
  arrange(mse)

fit_tab <- child_mfit %>% rename(child_mse = mse, child_beta = beta) %>%
  left_join(adult_mfit %>% rename(adult_mse = mse, adult_beta = beta)) %>% 
  arrange(adult_mse)

knitr::kable(fit_tab, digits = 3)
```



```{r}
fits <- child_mfit %>% mutate(AgeGroup = "children") %>% 
  bind_rows(adult_mfit %>% mutate(AgeGroup = "adults"))

save(mdat, mdat_eng, fits, file=here("models/model_data.RData"))
```

```{r, eval=F}
fits %>% ggplot(aes(x=beta, y=mse, color=AgeGroup)) +
  #geom_text(aes(label=feature)) + 
  geom_point(alpha=.5) +
  theme_minimal()
```
So generally, children's choices are best explained by small values of beta (.5, the minimum tested).


What do the predicted choice proportions from the best feature for each age group look like?

```{r}
require(ggpubr)

plot_model_preds <- function(feat_name, fits, hum_data, age="adults", mdat) {
  beta = 1 #subset(fits, AgeGroup==age & feature==feat_name)$beta
  md <- get_model_target_choice_props(get_feature_df(mdat, feat_name), hum_data, beta)
  md %>% ggplot(aes(x=chose_target, y=Model_targ_prop, color=relation)) +
    geom_abline(slope=1, intercept=0, linetype="dashed") +
    geom_point(alpha=.5) + geom_smooth(method='lm') + 
    xlab("Human Choice Probability") + 
    ylab("Model Choice Probability") +
    ggtitle(paste0(feat_name,": ",age)) + theme_minimal() + 
    xlim(0,1) + ylim(0,1)
}

ad_supp <- plot_model_preds("support_probability", fits, adult_trial_agg, age="adults", mdat)
ch_supp <- plot_model_preds("support_probability", fits, child_trial_agg, age="children", mdat)

ggarrange(ad_supp, ch_supp, nrow=1, common.legend = T)
```


```{r}
ad <- plot_model_preds("support_combo", fits, adult_trial_agg, age="adults", mdat_eng)
ch <- plot_model_preds("support_combo", fits, child_trial_agg, age="children", mdat_eng)

ggarrange(ad, ch, nrow=1, common.legend = T)
```

```{r}
ad <- plot_model_preds("support_response_sharpness_C=1e-05", fits, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_response_sharpness_C=1e-05", fits, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```

```{r}
ad <- plot_model_preds("support_std", fits, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_std", fits, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```

```{r}
ad <- plot_model_preds("len_inverse_sharpe_ratio", fits, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("len_inverse_sharpe_ratio", fits, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```


```{r}
ad <- plot_model_preds("avg_final_radius_objects=target", fits, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("avg_final_radius_objects=target", fits, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```
