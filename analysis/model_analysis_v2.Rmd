---
title: "curiobaby_drop model analysis"
author: "George Kachergis"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
require(tidyverse)
require(here)
require(kableExtra)

FIT_MODELS = F
```

## Preprocessing

```{r, load-data, echo=F, eval=FIT_MODELS}

load_data <- function(fname) {
  raw <- read.table(here(paste0("models/",fname,".csv")), sep=',')
  cnames <- raw[1,]
  mdat <- raw[2:nrow(raw),]
  names(mdat) = cnames
  for(c in 1:(ncol(mdat)-3)) {
    mdat[,c] = as.numeric(mdat[,c])
    if(min(mdat[,c]) < 0) {
      mdat[,c] = mdat[,c] + min(mdat[,c])
      print(paste("shifting",c,"so minimum is 0"))
    }
  }
  mdat$trial = rep(1:20, each=2)
  # right now, each trial is 2 rows, 
  # uniquely identified with drop_object x condition
  #mdat %>% distinct(drop_object, condition)

  mdat <- mdat %>% rename(drop = drop_object,
                          target = target_object,
                          relation = condition) %>%
    mutate(drop = replace(drop, drop=="triangular_prism", "trig prism"),
         target = replace(target, target=="triangular_prism", "trig prism"))
  return(mdat)
}

# just 100 drops per trial
#mdat100 <- load_data("model_stats")

mdat <- load_data("model_stats250")

# model split-half correlations and stds
mdat_cor <- read_csv(here("models/model_stats250_corr.csv")) 
# avg_len NA, but all others ~.997 - .999
#View(mdat_cor)

mdat_std <- load_data("model_stats250_std")
mdat_std$drop = mdat$drop
mdat_std$target = mdat$target
mdat_std$relation = mdat$relation
mdat_std$trial = mdat$trial

sort(colMeans(mdat[,1:43]))
# mean of obj_final_position_invstd_objects=drop is negative..
# mean of avg_len is 235, and len_std is 45 (much larger than typical feature mean on order of .1 - 1.0)
# scale these cols? (only matters for their combination)
mdat$avg_len = mdat$avg_len / max(mdat$avg_len)
mdat$len_std = mdat$len_std / max(mdat$len_std)

# engineer some features: 
# combine target + drop target features
mdat_eng <- mdat %>%
  rowwise() %>%
  mutate(obj_final_position_std = mean(`obj_final_position_std_objects=drop`,
                                       `obj_final_position_std_objects=target`),
         normed_velocity_std_after_first_collision_objects = 
           mean(`normed_velocity_std_after_first_collision_objects=drop`,
                `normed_velocity_std_after_first_collision_objects=target`),
         avg_final_radius_objects = mean(`avg_final_radius_objects=drop`, 
                                         `avg_final_radius_objects=target`),
         avg_max_radius_objects = mean(`avg_max_radius_objects=drop`,
                                       `avg_max_radius_objects=target`),
         max_radius_std_objects = mean(`max_radius_std_objects=drop`,
                                       `max_radius_std_objects=target`),
         support_combo = mean(support_probability, support_std, 
                              `support_response_sharpness_C=1`, # add support_response_linearity_r or _pv ?
                              `support_response_sharpness_accuracy_C=1`),
         support_sharpness_mean = mean(`support_response_sharpness_C=1e-05`,
                                       `support_response_sharpness_C=0.0001`,
                                       `support_response_sharpness_C=0.001`,
                                       `support_response_sharpness_C=0.01`,
                                       `support_response_sharpness_C=0.1`,
                                       `support_response_sharpness_C=1`,
                                       `support_response_sharpness_C=10.0`,
                                       `support_response_sharpness_C=100.0`,
                                       `support_response_sharpness_C=1000.0`,
                                       `support_response_sharpness_C=10000.0`,
                                       `support_response_sharpness_C=100000.0`),
         support_sharpness_accuracy_mean = mean(
            `support_response_sharpness_accuracy_C=1e-05`,
             `support_response_sharpness_accuracy_C=0.0001`,
             `support_response_sharpness_accuracy_C=0.001`,
             `support_response_sharpness_accuracy_C=0.01`,
             `support_response_sharpness_accuracy_C=0.1`,
             `support_response_sharpness_accuracy_C=1`,
             `support_response_sharpness_accuracy_C=10.0`,
             `support_response_sharpness_accuracy_C=100.0`,
             `support_response_sharpness_accuracy_C=1000.0`,
             `support_response_sharpness_accuracy_C=10000.0`,
             `support_response_sharpness_accuracy_C=100000.0`),
         support_response_linearity_mean = mean(support_response_linearity_pv, 
                                                support_response_linearity_r),
         len_combo = mean(avg_len, len_std, len_inverse_sharpe_ratio)
         ) %>%
  select(obj_final_position_std, 
         normed_velocity_std_after_first_collision_objects, 
         avg_final_radius_objects,
         avg_max_radius_objects,
         max_radius_std_objects,
         support_combo, 
         support_sharpness_mean,
         support_sharpness_accuracy_mean,
         len_combo,
         drop, target, relation)

mdat <- mdat %>% left_join(mdat_eng)
```

# Load human data

And define helper functions

```{r, echo=F}
# selects given feature and returns wide df that can be compared with behavioral data
get_feature_df <- function(mdat, fname) {
  fdf <- mdat[,c(fname,"drop","target","relation")] %>%
    pivot_wider(id_cols = c("drop","relation"), names_from = "target", values_from = fname) %>%
    arrange(relation,drop)
}

load(file=here("paper/processed_data.RData")) 

# ToDo: hold out half the subjects and CV on other half
adult_trial_agg <- adult_long %>% group_by(relation, drop, target) %>% # alt, 
  summarize(chose_target=mean(chose_target, na.rm=T)) %>% # , n=n()
  arrange(relation, desc(chose_target))

#hum_ad <- adult_trial_agg %>%
#  pivot_wider(id_cols = c("drop","relation"), names_from = "target", values_from = chose_target) %>%
#  arrange(relation,drop)

softmax <- function(weights, beta=1, smooth=1e-7) {
  weights = weights + smooth
  probs = (weights / sum(weights)) # normalize first
  num <- exp(beta * probs)
  return(num / sum(num))
}
# softmax(c(.7,.3), beta=2)

get_model_target_choice_props <- function(feat_df, hum_df, beta) {
  hum_df <- hum_df %>% arrange(relation, drop) # same order as model df
  hum_df$Model_targ_prop = 0
  for(i in 1:nrow(hum_df)) {
    mod_cols = which(!is.na(feat_df[i,1:ncol(feat_df)]))[3:4] 
    mod_choice = softmax(feat_df[i,mod_cols], beta=beta)
    hum_df[i,]$Model_targ_prop = as.numeric(mod_choice[hum_df[i,]$target] / sum(mod_choice))
  }
  return(hum_df)
}


evalFit <- function(feat_df, hum_df, beta, cor_objective=F, relation=NA) {
  hum_df <- get_model_target_choice_props(feat_df, hum_df, beta)
  if(!is.na(relation)) {
    hum_df <- hum_df[which(hum_df$relation==relation),]
  }
  r = with(hum_df, cor(chose_target, Model_targ_prop))
  mse = with(hum_df, sum((chose_target - Model_targ_prop)^2)) / nrow(hum_df) 
  if(cor_objective) mse = mse - r # also try to minimize -correlation
  return(mse) # return negative cor since DEoptim minimizes
}

child_trial_agg <- child_long %>% group_by(relation, drop, target) %>% # alt, 
  summarize(chose_target=mean(chose_target, na.rm=T), n=n())
```


## Fit models to all trials (MSE objective)

Find best-fitting betas per feature for children and adults, contain and support trials together.

```{r fit-models-to-all-trials-mse, echo=F, eval=FIT_MODELS}
require(DEoptim)

all_trials_mse <- tibble()

for(feat in feat_names) {
  feat_df <- get_feature_df(mdat, feat)
  #paste(feat, evalFit(feat_df, adult_trial_agg, 1))
  adult_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=adult_trial_agg)
  child_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=child_trial_agg)
  all_trials_mse <- bind_rows(all_trials_mse, c(Age = "adults", feature=feat, beta=as.numeric(unlist(adult_fit$optim$bestmem)), 
                          mse = adult_fit$optim$bestval))
  all_trials_mse <- bind_rows(all_trials_mse, c(Age = "children", feature=feat, 
                          beta = as.numeric(unlist(child_fit$optim$bestmem)), 
                          mse = child_fit$optim$bestval))
}

all_trials_mse_r <- all_trials_mse_r %>% mutate(beta = as.numeric(beta),
                                                mse = as.numeric(mse))

```

## Fit models to all trials (MSE - r objective)


```{r fit-models-to-all-trials-mse-and-r, echo=F, eval=FIT_MODELS}
all_trials_mse_r <- tibble()

for(feat in feat_names) {
  feat_df <- get_feature_df(mdat, feat)
  #paste(feat, evalFit(feat_df, adult_trial_agg, 1))
  adult_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=adult_trial_agg, cor_objective=T)
  child_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=child_trial_agg, cor_objective=T)
  all_trials_mse_r <- bind_rows(all_trials_mse_r, c(Age = "adults", feature=feat, beta=as.numeric(unlist(adult_fit$optim$bestmem)), 
                          mse = adult_fit$optim$bestval))
  all_trials_mse_r <- bind_rows(all_trials_mse_r, c(Age = "children", feature=feat, 
                          beta = as.numeric(unlist(child_fit$optim$bestmem)), 
                          mse = child_fit$optim$bestval))
}

all_trials_mse_r <- all_trials_mse_r %>% mutate(beta = as.numeric(beta),
                                                mse = as.numeric(mse))
```

## Fit separately to contain and support trials (MSE)

```{r fit-models-to-supp-cont-trials-mse, echo=F, eval=FIT_MODELS}
supp_trials_mse <- tibble()
cont_trials_mse <- tibble()

for(feat in feat_names) {
  feat_df <- get_feature_df(mdat, feat)
  #paste(feat, evalFit(feat_df, adult_trial_agg, 1))
  adult_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=adult_trial_agg, relation="support")
  child_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=child_trial_agg, relation="support")
  supp_trials_mse <- bind_rows(supp_trials_mse, c(Age = "adults", feature=feat, beta=as.numeric(unlist(adult_fit$optim$bestmem)), 
                          mse = adult_fit$optim$bestval))
  supp_trials_mse <- bind_rows(supp_trials_mse, c(Age = "children", feature=feat, 
                          beta = as.numeric(unlist(child_fit$optim$bestmem)), 
                          mse = child_fit$optim$bestval))
}

supp_trials_mse <- supp_trials_mse %>% mutate(beta = as.numeric(beta),
                                                mse = as.numeric(mse))


for(feat in feat_names) {
  feat_df <- get_feature_df(mdat, feat)
  adult_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=adult_trial_agg, relation="contain")
  child_fit = DEoptim(evalFit, lower=.5, upper=10, DEoptim.control(reltol=.001, NP=30, itermax=20), 
                feat_df=feat_df, hum_df=child_trial_agg, relation="contain")
  cont_trials_mse <- bind_rows(cont_trials_mse, c(Age = "adults", feature=feat, beta=as.numeric(unlist(adult_fit$optim$bestmem)), 
                          mse = adult_fit$optim$bestval))
  cont_trials_mse <- bind_rows(cont_trials_mse, c(Age = "children", feature=feat, 
                          beta = as.numeric(unlist(child_fit$optim$bestmem)), 
                          mse = child_fit$optim$bestval))
}

cont_trials_mse <- cont_trials_mse %>% mutate(beta = as.numeric(beta),
                                                mse = as.numeric(mse))

save(mdat, all_trials_mse, all_trials_mse_r, 
     supp_trials_mse, cont_trials_mse, file=here("models/model_data.RData"))
```




```{r load-model-data, echo=F}
load(here("models/model_data.RData"))

feat_names = setdiff(names(mdat), c("drop","target","relation","trial"))
# for each feature, want to fit beta to maximize correlation between human and model choices
#feat_df <- get_feature_df(mdat, feat_names[1])
```


## Feature correlations

Some of these features are perfectly correlated and even have identical values: e.g. support_probability and support_combo.
All of the support_response_sharpness features with C<=.01 (C=.1 and above behave differently).
All of the support_response_sharpness_accuracy features with C<=.01 (C=.1 and above behave differently).
We remove some of these from the below tables.

```{r, echo=F, include=F}
get_feature_cor <- function(feat1, feat2, mdat) {
  supp_prob <- get_feature_df(mdat, feat1) %>%
    pivot_longer(3:12, names_to="target", values_drop_na=T)
  supp_combo <- get_feature_df(mdat, feat2) %>%
    pivot_longer(3:12, names_to="target", values_drop_na=T)
  return(list(r=cor(supp_combo$value, supp_prob$value), SSE=sum((supp_combo$value - supp_prob$value)^2)))
}

get_feature_cor("support_probability", "support_combo", mdat)
# same (1)

# support_response_sharpness_C=0.01 or C=.001 almost as good for children
get_feature_cor("support_probability", "support_response_sharpness_C=0.01", mdat)
# .91

get_feature_cor("support_response_sharpness_C=0.001", "support_response_sharpness_C=0.01", mdat) # 1 
get_feature_cor("support_response_sharpness_C=0.001", "support_response_sharpness_C=0.0001", mdat) # 1
get_feature_cor("support_response_sharpness_C=0.001", "support_response_sharpness_C=1e-05", mdat) # 1

get_feature_cor("support_response_sharpness_C=0.1", "support_response_sharpness_C=0.01", mdat) # .95
get_feature_cor("support_response_sharpness_C=0.1", "support_response_sharpness_C=1", mdat) # .94

get_feature_cor("support_response_sharpness_accuracy_C=0.001", "support_response_sharpness_accuracy_C=0.01", mdat) # 1
get_feature_cor("support_response_sharpness_accuracy_C=0.0001", "support_response_sharpness_accuracy_C=0.01", mdat)

# remove some of the identical features
all_trials_mse <- all_trials_mse %>% filter(feature!="support_combo",
                                            feature!="support_sharpness_mean",
                                            feature!="support_response_sharpness_C=0.0001", # keep .01
                                            feature!="support_response_sharpness_C=0.001",
                                            feature!="support_response_sharpness_C=1e-05",
                                            feature!="support_response_sharpness_accuracy_C=0.0001",
                                            feature!="support_response_sharpness_accuracy_C=0.001",
                                            feature!="support_response_sharpness_accuracy_C=1e-05")
all_trials_mse_r <- all_trials_mse_r %>% filter(feature!="support_combo",
                                            feature!="support_sharpness_mean",
                                            feature!="support_response_sharpness_C=0.0001", # keep .01
                                            feature!="support_response_sharpness_C=0.001",
                                            feature!="support_response_sharpness_C=1e-05",
                                            feature!="support_response_sharpness_accuracy_C=0.0001",
                                            feature!="support_response_sharpness_accuracy_C=0.001",
                                            feature!="support_response_sharpness_accuracy_C=1e-05")
supp_trials_mse <- supp_trials_mse %>% filter(feature!="support_combo",
                                            feature!="support_sharpness_mean",
                                            feature!="support_response_sharpness_C=0.0001", # keep .01
                                            feature!="support_response_sharpness_C=0.001",
                                            feature!="support_response_sharpness_C=1e-05",
                                            feature!="support_response_sharpness_accuracy_C=0.0001",
                                            feature!="support_response_sharpness_accuracy_C=0.001",
                                            feature!="support_response_sharpness_accuracy_C=1e-05")
cont_trials_mse <- cont_trials_mse %>% filter(feature!="support_combo",
                                            feature!="support_sharpness_mean",
                                            feature!="support_response_sharpness_C=0.0001", # keep .01
                                            feature!="support_response_sharpness_C=0.001",
                                            feature!="support_response_sharpness_C=1e-05",
                                            feature!="support_response_sharpness_accuracy_C=0.0001",
                                            feature!="support_response_sharpness_accuracy_C=0.001",
                                            feature!="support_response_sharpness_accuracy_C=1e-05")
```



We have `r length(feat_names)` features from 250 simulated drops per combination to optimize softmax betas for.
The tables below show fits and betas for the best-fitting 10 features (sorted by adult fit).

## Fits to all trials

```{r, echo=F}
fit_tab <- all_trials_mse %>% pivot_wider( names_from = Age, values_from = c(beta, mse)) %>%
  arrange(mse_adults)

knitr::kable(fit_tab[1:10,], digits = 3)
```


## Fits to all trials (MSE - r)

```{r, echo=F}
all_trials_mse_r <- all_trials_mse_r %>% mutate(beta = as.numeric(beta),
                                                mse = as.numeric(mse))

fit_tab <- all_trials_mse_r %>% pivot_wider( names_from = Age, values_from = c(beta, mse)) %>%
  arrange(mse_adults)

knitr::kable(fit_tab[1:10,], digits = 3)
```


## Fits to support trials (MSE)

```{r, echo=F}
fit_tab <- supp_trials_mse %>% pivot_wider(names_from = Age, values_from = c(beta, mse)) %>%
  arrange(mse_adults)

knitr::kable(fit_tab[1:10,], digits = 3)
```

## Fits to containment trials (MSE)

```{r, echo=F}
fit_tab <- cont_trials_mse %>% pivot_wider(names_from = Age, values_from = c(beta, mse)) %>%
  arrange(mse_adults)

knitr::kable(fit_tab[1:10,], digits = 3)
```


What do the predicted choice proportions from the best features for each age group look like?
Let's look at the best feature for each each group from each fit method.

```{r plot-model-vs-human, echo=F, message=F}
require(ggpubr)

plot_model_preds <- function(feat_name, fits, hum_data, age="adults", mdat) {
  this_dat = subset(fits, Age==age & feature==feat_name)
  md <- get_model_target_choice_props(get_feature_df(mdat, feat_name), hum_data, this_dat$beta)
  md %>% ggplot(aes(x=chose_target, y=Model_targ_prop, color=relation)) +
    geom_abline(slope=1, intercept=0, linetype="dashed") +
    geom_point(alpha=.5) + geom_smooth(method='lm', formula=y ~ x) + 
    xlab("Human Choice Probability") + 
    ylab("Model Choice Probability") +
    ggtitle(paste0(age,": ",feat_name)) + theme_minimal() + 
    xlim(0,1) + ylim(0,1) +
    annotate("text", x=.5, y=0.05, 
             label = paste0("beta = ",round(this_dat$beta,2),", MSE = ",round(this_dat$mse, 3))) # 
}
```

## Best feature from fitting all trials (MSE)

When fit with MSE, the best-fitting feature for both children and adults is support probability, shown below with betas optimized separately for adults' and children's data.
Note that although the MSE is quite low, the correlation between model and human responding for each trial is negative.

```{r}
get_model_error_bars <- function(feat_name, mdat_std) {
  # have std per drop, so we avg per trial (?) 
  # (could get SEM for N=250 drops ?)
  mdat_std <- mdat_std %>% arrange(relation, drop)
  sd_trial <- rowMeans(matrix(mdat_std[,feat_name], nrow=20, byrow=T))
  return(sd_trial)
}

ch_feat <- all_trials_mse %>% filter(Age=="children") %>%
  arrange(mse) 

ad_feat <- all_trials_mse %>% filter(Age=="adults") %>%
  arrange(mse) 

ad_df = get_model_target_choice_props(get_feature_df(mdat, "support_probability"), adult_trial_agg, ad_feat[2,]$beta)
ad_df$model_sd = get_model_error_bars("support_probability", mdat_std)

ch_df = get_model_target_choice_props(get_feature_df(mdat, "support_probability"), child_trial_agg, ch_feat[1,]$beta)
ch_df$model_sd = get_model_error_bars("support_probability", mdat_std)


supp_prob_beta1 <- 
  bind_rows(get_model_target_choice_props(get_feature_df(mdat, "support_probability"), adult_trial_agg, beta=1) %>% mutate(AgeGroup="adults"),
            get_model_target_choice_props(get_feature_df(mdat, "support_probability"), child_trial_agg, beta=1) %>% mutate(AgeGroup="children") %>% select(-n))



supp_resp_sharp_beta1 <- 
  bind_rows(get_model_target_choice_props(get_feature_df(mdat, "support_response_sharpness_C=0.01"), adult_trial_agg, beta=1) %>% mutate(AgeGroup="adults"),
            get_model_target_choice_props(get_feature_df(mdat, "support_response_sharpness_C=0.01"), child_trial_agg, beta=1) %>% mutate(AgeGroup="children") %>% select(-n))

supp_resp_sharp_beta_opt <- 
  bind_rows(get_model_target_choice_props(get_feature_df(mdat, "support_response_sharpness_C=0.01"), adult_trial_agg, beta=1.94) %>% mutate(AgeGroup="adults"),
            get_model_target_choice_props(get_feature_df(mdat, "support_response_sharpness_C=0.01"), child_trial_agg, beta=0.810) %>% mutate(AgeGroup="children") %>% select(-n))

save(ad_df, ch_df, supp_prob_beta1, supp_resp_sharp_beta1, supp_resp_sharp_beta_opt,
     file=here("models/support_probability_fit_df.RData"))

supp_std_prob_cor <- get_feature_cor("support_std", "support_probability", mdat) # .72

supp_resp_sharp_prob_cor <- get_feature_cor("support_response_sharpness_C=0.01", "support_probability", mdat)

ad <- plot_model_preds("support_probability", all_trials_mse, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_probability", all_trials_mse, child_trial_agg, age="children", mdat)
# ToDo: add error bars to each trial for model (from mdat_std) and from humans...

ggarrange(ad, ch, nrow=1, common.legend = T)
```

### How much of the total explainable variance (e.g. relative to measurement noise) is explained by the best model for the adult data?

```{r}
#md <- get_model_target_choice_props(get_feature_df(mdat, feat_name), hum_data, this_dat$beta)

```


The next-best feature for adults was support_std, which has r=`r round(supp_std_prob_cor$r, 2)` with the support_probability feature.
The next-best feature for children was support_response_sharpness_C=0.01 (or C=.001, .0001, or 1e-05), which has r=`r round(supp_resp_sharp_prob_cor$r, 2)` with the support_probability feature.
These second-best features are shown below.

```{r, echo=F, message=F}
ad <- plot_model_preds("support_std", all_trials_mse, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_response_sharpness_C=0.01", all_trials_mse, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```

# Best feature from fitting all trials (MSE - r)

When fit with a combined MSE and r objective, the best-fitting feature for both children and adults is support_response_sharpness_C=0.01, shown below with betas optimized separately for adults' and children's data.

```{r}
ch_feat <- all_trials_mse_r %>% filter(Age=="children") %>%
  arrange(mse) # support_response_sharpness_C=0.01, then support_response_sharpness_C=0.1 

ad_feat <- all_trials_mse_r %>% filter(Age=="adults") %>%
  arrange(mse) # support_response_sharpness_C=0.01, then normed_velocity_std_after_first_collision_objects=target

supp_res_sharp_pt01_pt1 <- get_feature_cor("support_response_sharpness_C=0.01", "support_response_sharpness_C=0.1", mdat) # .95

supp_resp_sharp_norm_vel_targ <- get_feature_cor("support_response_sharpness_C=0.01", "normed_velocity_std_after_first_collision_objects=target", mdat)

ad <- plot_model_preds("support_response_sharpness_C=0.01", all_trials_mse_r, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_response_sharpness_C=0.01", all_trials_mse_r, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```

The next-best feature for adults was normed_velocity_std_after_first_collision_objects=target, which has r=`r round(supp_resp_sharp_norm_vel_targ$r, 2)` with the support_response_sharpness_C=0.01 feature.
The next-best feature for children was support_response_sharpness_C=0.1, which has r=`r round(supp_res_sharp_pt01_pt1$r, 2)` with the support_response_sharpness_C=0.01 feature.
These second-best features are shown below.

```{r, echo=F, message=F}
ad <- plot_model_preds("normed_velocity_std_after_first_collision_objects=target", all_trials_mse_r, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_response_sharpness_C=0.1", all_trials_mse_r, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```


## Best features from fitting containment trials (MSE)

The best-fitting feature from fitting only containment trials was support_probability, for both children and adults:

```{r}
ch_feat <- cont_trials_mse %>% filter(Age=="children") %>%
  arrange(mse) %>% slice(1) # support_probability then support_response_sharpness_C=10.0

ad_feat <- cont_trials_mse %>% filter(Age=="adults") %>%
  arrange(mse) %>% slice(1) # support_probability then support_std (beta=10 !)

knitr::kable(bind_rows(ch_feat, ad_feat), digits=3)
```


## Best features from fitting support trials (MSE)

The best-fitting feature for support trials for children was normed_velocity_std_after_first_collision_objects, and for adults was support_response_linearity_r (although support_probability was not far behind for adults):

```{r}
ch_feat <- supp_trials_mse %>% filter(Age=="children") %>%
  arrange(mse) %>% slice(1) #

ad_feat <- supp_trials_mse %>% filter(Age=="adults") %>%
  arrange(mse) %>% slice(1) # support_probability then support_std (beta=10 !)

bind_rows(ch_feat, ad_feat) %>% kable(digits=3)
```




### Other promising features: avg_final_radius_objects=target

avg_final_radius_objects=target shows positive correlations with children's and adults' preferences, but for many of the trials (both contain and support) the model chooses the target with small probability.

```{r,  echo=F, message=F}
ad <- plot_model_preds("avg_final_radius_objects=target", all_trials_mse, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("avg_final_radius_objects=target", all_trials_mse, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```

### Other promising features: support_response_sharpness_C=0.01

support_response_sharpness_C=0.01 has a fairly good MSE in particular for children, and also shows a positive correlation with children's preferences on containment trials.

```{r,  echo=F, message=F}
ad <- plot_model_preds("support_response_sharpness_C=0.01", all_trials_mse, adult_trial_agg, age="adults", mdat)
ch <- plot_model_preds("support_response_sharpness_C=0.01", all_trials_mse, child_trial_agg, age="children", mdat)

ggarrange(ad, ch, nrow=1, common.legend = T)
```
